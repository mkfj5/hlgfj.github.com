{
  "data": {
    "lesson": {
      "id": 559338,
      "key": "e3d2343d-a663-407a-9bbb-9a00504f7826",
      "title": "Additional Adversarial Search Topics",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Introduce Monte Carlo Tree Search, a highly-successful search technique in game domains, along with a reading list for other advanced adversarial search topics.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": null,
      "resources": null,
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 542382,
          "key": "809df0e6-4aef-49b0-801d-6f97fece7f57",
          "title": "Improving Minimax",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "809df0e6-4aef-49b0-801d-6f97fece7f57",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 542383,
              "key": "2e16171d-81a6-4bb4-a2dd-9d170551089f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Modern Variants of Minimax\n---\nMinimax with alpha beta pruning is useful - especially as a teaching tool - because it provides strong theoretical guarantees about selecting optimal moves, but in most real-world applications we're often willing to trade some of those guarantees for better empirical performance. There are several variants of Minimax that change or relax different assumptions in order to achieve stronger performance.\n\nWe will not cover these algorithms in detail (they rely on the same principles you've already seen). You are encouraged to experiment with these variants \n\n- [Negamax](https://en.wikipedia.org/wiki/Negamax) - the minimax algorithm can be simplified for [zero-sum games](https://en.wikipedia.org/wiki/Zero-sum_game)\n\n- [Principle Variation Search](https://en.wikipedia.org/wiki/Principal_variation_search) (i.e., Negascout) - combining negamax with good move ordering determined by null window searches to outperform alpha-beta pruning (i.e., start with a wide cutoff window and shrink it)\n\n- [Memory-enhanced Test Driver family](https://arxiv.org/pdf/1404.1515.pdf) (e.g., [MTD-f](https://en.wikipedia.org/wiki/MTD-f)) - performs a series of searches starting with only null window searches while keeping a memory store of visited states (i.e., start with a zero-width window and grow it); empirically shown to be the most efficient minimax search\n\n- [Best Node Search](https://en.wikipedia.org/wiki/Best_Node_Search) - repeatedly guess and check values for the bounds on the minimax value of each branch; empirically shown to be the most efficient minimax algorithm on random trees\n\n## Going Beyond Minimax\n---\nAdversarial Search has been a very active topic in the past several years. Adversarial\n\n- Monte Carlo Tree Search\n- Reinforcement Learning policy",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 559356,
          "key": "1ecef15d-82f5-4078-8f52-8d6424ee2a28",
          "title": "Monte Carlo Tree Search",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1ecef15d-82f5-4078-8f52-8d6424ee2a28",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624553,
              "key": "89336212-8a65-455d-ab5f-85e78c22a0e4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Monte Carlo Tree Search\n---\nMonte Carlo Tree Search is an alternative to Minimax search that has been used successfully in many modern applications. MCTS is a general search technique that can be trivially extended to adversarial search. The main benefit of MCTS vs Minimax is that it is an aheuristic search—you do not need a good search heuristic in the domain to get reasonable results, and it works better than minimax in extremely large domains (most Go agents—including AlphaGo and AlphaZero—use MCTS).",
              "instructor_notes": ""
            },
            {
              "id": 624556,
              "key": "881faf53-5fd9-4f68-8890-48d398237ef2",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aef9935_screen-shot-2018-05-06-at-5.07.33-pm/screen-shot-2018-05-06-at-5.07.33-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/881faf53-5fd9-4f68-8890-48d398237ef2",
              "caption": "",
              "alt": "",
              "width": 707,
              "height": 313,
              "instructor_notes": null
            },
            {
              "id": 624557,
              "key": "e3d2723e-0ead-46f5-9fab-ae11d708e4e9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## How it Works\n\nMCTS works by combining tree search with simulation; rather than using a heuristic to estimate the value of each leaf node of the search tree, it instead quickly simulates the remaining game according to some default policy, and uses many replications of those rollouts to estimate the value of each possible action. The MCTS algorithm includes four phases: Selection, Expansion, Simulation, and Backpropagation. Pseudocode for each phase is shown below, along with the backup step for 2-player games.\n\nIt is possible to perform MCTS with uniform sampling of the action space, but that tends to be inefficient in terms of sampling. Upper Confidence Bound for Trees (UCT) is an alternative sampling strategy that tries to sample more promising actions more frequently than other actions. It has been shown that MCTS converges to the minimax value of a search tree as the number of simulations goes towards infinity.",
              "instructor_notes": ""
            },
            {
              "id": 624554,
              "key": "a639597a-cf92-4f90-b024-d47a6332cf4f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aef9922_screen-shot-2018-05-06-at-5.07.48-pm/screen-shot-2018-05-06-at-5.07.48-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/a639597a-cf92-4f90-b024-d47a6332cf4f",
              "caption": "",
              "alt": "",
              "width": 353,
              "height": 575,
              "instructor_notes": null
            },
            {
              "id": 624555,
              "key": "82e0aaeb-adc5-438d-b85f-2c62d09d5f4a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aef992b_screen-shot-2018-05-06-at-5.07.58-pm/screen-shot-2018-05-06-at-5.07.58-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/82e0aaeb-adc5-438d-b85f-2c62d09d5f4a",
              "caption": "",
              "alt": "",
              "width": 354,
              "height": 128,
              "instructor_notes": null
            },
            {
              "id": 663380,
              "key": "c695f395-c3b5-4d9b-ae26-542add790a44",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### The MCTS Reward Function\n---\nThe reward function in MCTS does not rely on a heuristic, but the semantics of the reward can be confusing. In the formulation presented in the pseudocode above, the default policy should return **+1** if the agent holding initiative at the start of a simulation _loses_ and **-1** if the active agent when the simulation starts _wins_ because nodes store the reward _relative to their parent in the game tree_.\n\n#### But..._why?_\nThe BestChild() function chooses the action _a_ that maximizes _Q_ over the child nodes _v'_ of the input node _v_, so the value of Q should be higher if taking action _a_ from state _v_ will lead to the player with initiative in state _v_ (the parent) winning from state _v'_ (the child), and lower if taking the action will lead to a loss.",
              "instructor_notes": ""
            },
            {
              "id": 663382,
              "key": "136a9746-bdad-4fd9-aa22-f44ef7f5ef17",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/June/5b2c1c9a_mcts-rewards/mcts-rewards.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/136a9746-bdad-4fd9-aa22-f44ef7f5ef17",
              "caption": "",
              "alt": "",
              "width": 600,
              "height": 464,
              "instructor_notes": null
            },
            {
              "id": 663381,
              "key": "0a67d749-06a1-4260-9964-f0e64b66aaa0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The figure above illustrates the simplest representation of the four possible cases for new node expansion. \n\n1. Starting from _v=v<sub>0</sub>_, search may expand action _a<sub>0,0</sub>_ to reach _v<sub>1</sub>_, where the simulation results in a win for player 1. Notice that player **2** has initiative on the board when the simulation is performed, so the utility of the final state to the active player in state _v<sub>1</sub>_ (i.e., player 2) is -∞, however the MCTS reward should be +1 because player 1 benefits from choosing action _a<sub>0,0</sub>_.\n\n2. Starting from _v=v<sub>0</sub>_, search may expand action _a<sub>0,1</sub>_ to reach _v<sub>2</sub>_, where the simulation results in a win for player 2. The utility of the final simulation state to the active player in state _v<sub>2</sub>_ (i.e., player 2) is ∞, however the MCTS reward should be -1 in order to discourage Player 1 from choosing action _a<sub>0,1</sub>_.\n\n3. Starting from _v=v<sub>0</sub>_, search may expand action _a<sub>0,2</sub>_ to reach _v<sub>3</sub>_, then expands action _a<sub>3,0</sub>_ to reach _v<sub>4</sub>_ where the simulation results in a win for player 1. The utility of the final simulation state to the active player in the initial simulation state _v<sub>4</sub>_ (i.e., player 1) is ∞, however the MCTS reward should be -1 in order to discourage Player 2 (the parent of _v<sub>4</sub>_) from choosing action _a<sub>3,0</sub>_; the BackupNegamax() function inverts the reward before propagating back to _v<sub>3</sub>_ which makes Player 1 _more_ likely to choose action _a<sub>3,0</sub>_—which makes sense because the simulation on this branch led to a win for Player 1.\n\n4. Starting from _v=v<sub>0</sub>_, search may expand action _a<sub>0,2</sub>_ to reach _v<sub>3</sub>_, then expands action _a<sub>3,1</sub>_ to reach _v<sub>5</sub>_ where the simulation results in a win for player 2. The utility of the final simulation state to the active player in the initial simulation state _v<sub>4</sub>_ (i.e., player 1) is -∞, however the MCTS reward should be 1 in order to encourage Player 2 (the parent of _v<sub>5</sub>_) to choosing action _a<sub>3,1</sub>_; the BackupNegamax() function inverts the reward before propagating back to _v<sub>3</sub>_ which makes Player 1 _less_ likely to choose action _a<sub>3,0</sub>_—which makes sense because the simulation on this branch led to a loss for Player 1.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 593094,
          "key": "4fe6ee55-ce78-46d1-8183-69ac2891be92",
          "title": "Reading List",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "4fe6ee55-ce78-46d1-8183-69ac2891be92",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 593096,
              "key": "74615953-c072-487f-b7e1-5e4d4d201910",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Reading\n- AlphaGo\n - [AlphaGo: Mastering Go without Human Knowledge](https://deepmind.com/research/publications/mastering-game-go-without-human-knowledge/)\n - [Understanding AlphaGo](https://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0) & [Example Implementation](https://github.com/AppliedDataSciencePartners/DeepReinforcementLearning)\n - [AlphaZero: Mastering Chess & Shogi](https://arxiv.org/pdf/1712.01815.pdf)\n- [Monte Carlo Tree Search](https://www.cs.mcgill.ca/~vkules/bandits.pdf)\n- [A Survey of Monte Carlo Tree Search Methods](http://mcts.ai/pubs/mcts-survey-master.pdf)\n\n",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}