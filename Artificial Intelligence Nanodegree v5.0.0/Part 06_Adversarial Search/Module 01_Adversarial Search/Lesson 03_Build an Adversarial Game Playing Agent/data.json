{
  "data": {
    "lesson": {
      "id": 594207,
      "key": "ad737447-8247-4e0f-b44e-0ebf13e7be95",
      "title": "Build an Adversarial Game Playing Agent",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Extend classical search to adversarial domains, to build agents that make good decisions without any human interventionâ€”such as the DeepMind AlphaGo agent.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": null,
      "resources": null,
      "project": {
        "key": "3aa4a8e9-836c-4983-a884-ab981ecc4666",
        "version": "1.0.0",
        "locale": "en-us",
        "duration": 36000,
        "semantic_type": "Project",
        "title": "Build an Adversarial Game Playing Agent",
        "description": "## Synopsis\n\nIn this project, you will experiment with adversarial search techniques by building an agent to play knights Isolation. Unlike the examples in the lecture where the players control tokens that move like chess queens, this version of Isolation gives each agent control over a single token that moves in L-shaped movements--like a knight in chess.\n\n### Isolation \n\nIn the game Isolation, two players each control their own single token and alternate taking turns moving the token from one cell to another on a rectangular grid. Whenever a token occupies a cell, that cell becomes blocked for the remainder of the game. An open cell available for a token to move into is called a \"liberty\". The first player with no remaining liberties for their token loses the game, and their opponent is declared the winner.\n\nIn knights Isolation, tokens can move to any open cell that is 2-rows and 1-column or 2-columns and 1-row away from their current position on the board. On a blank board, this means that tokens have at most eight liberties surrounding their current location. Token movement is blocked at the edges of the board (the board does not wrap around the edges), however, tokens can \"jump\" blocked or occupied spaces (just like a knight in chess).\n\nFinally, agents have a fixed time limit (150 milliseconds by default) to search for the best move and respond. The search will be automatically cut off after the time limit expires, and the active agent will forfeit the game if it has not chosen a move.\n\n## Getting Started (Workspaces)\n\nThe easiest way to complete the project is to use the Udacity Workspace in your classroom. The environment has already been configured with the required files and libraries to support the project. If you decide to use the Workspace, then you do NOT need to perform any of the setup steps for this project. Skip to the section with instructions for completing the project.\n\n## Getting Started (Local Environment)\n\nIf you would prefer to complete the exercise in your own local environment, then follow the steps below:\n\n-   Open your terminal and activate the aind conda environment (OS X or Unix/Linux users use the command shown; Windows users only run  `activate aind`)\n    \n    ```\n    $ source activate aind\n    \n    ```\n    \n-   Download a copy of the project files from GitHub and navigate to the project folder. (Note: if you've previously downloaded the repository for another project then you can skip the clone command. However, you should run  `git pull`  to receive any project updates since you cloned the repository.)\n    \n    ```\n    (aind) $ git clone https://github.com/udacity/artificial-intelligence\n    (aind) $ cd \"artificial-intelligence/Projects/3_Game Playing\"\n    \n    ```\n    \n\n## Instructions\n\nYou must implement an agent in the  `CustomPlayer`  class defined in the  `game_agent.py`  file. The interface definition for game agents only requires you to implement the  `.get_action()`  method, but you can add any other methods to the class that you deem necessary. You can build a basic agent by combining minimax search with alpha-beta pruning and iterative deepening from the lecture.\n\n**NOTE:**  Your agent will  **not**  be evaluated in an environment suitable for running machine learning or deep learning agents (like AlphaGo).\n\n#### The get_action() Method\n\nThis function is called once per turn for each player. The calling function handles the time limit and\n\n```\ndef get_action(self, state):\n    import random\n    self.queue.put(random.choice(state.actions()))\n\n```\n\n-   **DO NOT**  use multithreading/multiprocessing (the isolation library already uses them, which may cause conflicts)\n-   **ALL**  of the functions you add should be created as methods on the CustomPlayer class.\n\n#### Initialization Data\n\nYour agent will automatically read the contents of a file named  `data.pickle`  if it exists in the same folder as  `my_custom_player.py`. The serialized object from the pickle file will be assigned to  `self.data`. Your agent should not write to or modify the contents of the pickle file during the search.\n\n#### Saving Information Between Turns\n\nThe  `CustomPlayer`  class can pass the internal state by assigning the data to the attribute  `self.context`. An instance of your agent class will carry the context between each turn of a single game, but the contents will be reset at the start of any new game.\n\n```\ndef get_action(...):\n    action = self.mcts()\n    self.queue.put(action)\n    self.context = object_you_want_to_save  # self.context will contain this object on the next turn\n\n```\n\n## Pick an Experiment\n\nSelect at least one of the following to implement and evaluate in your report. (There is no upper limit on the techniques you incorporate into your agent.)\n\n### Option 1: Develop a custom heuristic (must not be one of the heuristics from lectures, and cannot only be a combination of the number of liberties available to each agent)\n\n-   Create a performance baseline using  `run_search.py`  (with the  `fair_matches`  flag enabled) to evaluate the effectiveness of your agent using the #my_moves - #opponent_moves heuristic from lecture\n-   Use the same process to evaluate the effectiveness of your agent using your own custom heuristic\n\n**Hints:**\n\n-   Research other games (chess, go, connect4, etc.) to get ideas for developing good heuristics\n-   If the results of your tests are very close, try increasing the number of matches (e.g., >100) to increase your confidence in the results\n-   Experiment with adding more search time--does adding time confer any advantage to your agent over the baseline?\n-   Augment the code to count the number of nodes your agent searches--is it better to search more or fewer nodes? How does your heuristic compare to the baseline heuristic you chose?\n\n### Option 2: Develop an opening book (must span at least depth 4 of the search tree)\n\n-   Write your own code to develop an opening book of the best moves for every possible game state from an empty board to at least a depth of 4 plies\n-   Create a performance baseline using  `run_search.py`  (with the  `fair_matches`  flag  _disabled_) to evaluate the effectiveness of your agent using randomly chosen opening moves. (You can use any heuristic function, but you should use the same heuristic on your agent for all experiments.)\n-   Use the same procedure to evaluate the effectiveness of your agent when early moves are selected from your opening book\n\n**Hints:**\n\n-   Developing an opening book can require long run-times to simulate games and accumulate outcome statistics\n-   If the results are very close, try increasing the number of matches (e.g., >100) to increase your confidence in the results\n\n**Adding a basic opening book**\n\n-   You will need to write your own code to develop a good opening book, but you can pass data into your agent by saving the file as \"data.pickle\" in the same folder as  `game_agent.py`. Use the  [pickle](https://docs.python.org/3/library/pickle.html)module to serialize the object you want to save. The pickled object will be accessible to your agent through the  `self.data`  attribute.\n\nFor example, the contents of dictionary  `my_data`  can be saved to disk:\n\n```\nimport pickle\nfrom isolation import Isolation\nstate = Isolation()\nmy_data = {state: 57}  # opening book always chooses the middle square on an open board\nwith open(\"data.pickle\", 'wb') as f:\n    pickle.dump(my_data, f)\n\n```\n\n### Option 3: Build an agent using advanced search techniques (for example killer heuristic, principal variation search (not in lecture), or monte carlo tree search (not in lecture))\n\n-   Create a performance baseline using  `run_search.py`  to evaluate the effectiveness of a baseline agent (e.g., an agent using your minimax or alpha-beta search code from the classroom)\n-   Use  `run_search.py`  to evaluate the effectiveness of your agent using your own custom search techniques\n-   You must decide whether to test with or without \"fair\" matches enabled--justify your choice in your report\n\n**Hints:**\n\n-   If the results are very close, try increasing the number of matches (e.g., >100) to increase your confidence in the results\n-   Experiment with adding more search time--does adding time confer any advantage to your agent?\n-   Augment the code to count the number of nodes your agent searches--does your agent have an advantage compared to the baseline search algorithm you chose?\n\n**Note:**\n\n-   You MAY implement advanced techniques from the reading list at the end of the lesson (like Monte Carlo Tree Search, principal variation search, etc.), but your agent is being evaluated for  _performance_rather than  _correctness_. It's possible to pass the project requirements  **without**  using these advanced techniques, so project reviewers may encourage you to implement a simpler solution if you are struggling with the correct implementation. (That's good general advice: do the simplest thing first, and only add complexity when you must.)\n\n## Report Requirements\n\nYour report must include a table or chart with data from an experiment to evaluate the performance of your agent as described above. Use the data from your experiment to answer the relevant questions below. (You may choose one set of questions if your agent incorporates multiple techniques.)\n\n**Advanced Heuristic**\n\n-   What features of the game does your heuristic incorporate, and why do you think those features matter in evaluating states during the search?\n-   Analyze the search depth your agent achieves using your custom heuristic. Does search speed matter more or less than accuracy to the performance of your heuristic?\n\n**Opening book**\n\n-   Describe your process for collecting statistics to build your opening book. How did you choose states to sample? And how did you perform rollouts to determine a winner?\n-   What opening moves does your book suggests are most effective on an empty board for player 1 and what is player 2's best reply?\n\n**Advanced Search Techniques**\n\n-   Choose a baseline search algorithm for comparison (for example, alpha-beta search with iterative deepening, etc.). How much performance difference does your agent show compared to the baseline?\n-   Why do you think the technique you chose was more (or less) effective than the baseline?\n\n## Evaluation\n\nYour project will be reviewed by a Udacity reviewer against the project rubric  [here](https://review.udacity.com/#!/rubrics/1801/view). Review this rubric thoroughly, and self-evaluate your project before submission. All criteria found in the rubric must meet specifications for you to pass.\n\n## Self Test  \nIn the section **Submission**  you will find instructions to submit your project first to an auto-grader and eventually to a Udacity reviewer in the following sections. Before you proceed to the section it is a good idea to run some sanity checks on your code.   \n\nTo perform the sanity check, from the terminal switch to directory containing your project files and follow the instructions below:\n\n1. ### Test Your Code Against An Agent  \n   Run the following command:\n    ```  \n    $ run_match.py -o SELF  \n   ```  \n    You can also choose GREEDY, RANDOM and MINIMAX as agents as your opponent. Make sure does not produce any errors.  \n2. ### Local Unit Test Cases:  \n    Run the following command:  \n\t ```  \n    $ python -m unittest\n\t```\n\tOnce again, ensure that there are no errors.\nIf there are errors, in either of the steps, use the instruction in the **Debug Your Agent** below to get more information.\n\n\n## Debugging Your Agent\nIf you encounter an exception or a failed test case in either of the steps above, use the instruction below:\n- In  **my_custom_agent.py**  add the following line:\n    ```\n    from isolation import DebugState\n  ```\n- In your implementation of **get_action()**, add the following three lines:\n    ```\n    print('In get_action(), state received:')\n    debug_board = DebugState.from_state(state)\n   print(debug_board)\n    ```\n\nNow when you run a game using either the `run_match.py` script or by executing `python -m unittest`, the state of the game in the form of a nicely formatted board that looks like this:\n\n![Adversarial-Search](https://video.udacity-data.com/topher/2020/January/5e1593e7_screenshot-from-2020-01-08-14-02-19/screenshot-from-2020-01-08-14-02-19.png) \n\nThe `X's` are positions that are already blocked and `1` and `2` represent the current positions of the competing agents. You can use this strategy to zero in the scenarios where the error(s) happen.\n\n**Once you have fixed the errors please make sure you delete or comment out the debugging code.**\n\n## Submission\n\nBefore you can submit your project for review in the classroom, you must run the remote test suite & generate a zip archive of the required project files. Submit the archive in your classroom for review. (See notes on submissions below for more details.) From your terminal, run the command: (make sure to activate the aind conda environment if you're running the project in your local environment; workspace users do  **not**  need to activate an environment.)\n\n```\n$ udacity submit\n\n```\n\nThe script will automatically create a zip archive of the required files (`my_custom_player.py`  and  `report.pdf`  are required;  `data.pickle`  will be included if it exists) and submit your code to a remote server for testing. You can only submit a zip archive created by the PA script (even if you're only submitting a partial solution), and you  **must submit the exact zip file created by the Project Assistant**  in your classroom for review. The classroom verifies the zip file submitted against records on the Project Assistant system; any changes in the file will cause your submission to be rejected.\n\n**NOTE:**  Students who authenticate with Facebook or Google accounts  _must_  follow the instructions on the FAQ page  [here](https://project-assistant.udacity.com/faq)  to obtain an authentication token. (The Workspace already includes instructions for obtaining and configuring your token.)\n",
        "is_public": true,
        "summary": null,
        "forum_path": "",
        "rubric_id": "1801",
        "terminal_project_id": null,
        "resources": null,
        "image": null
      },
      "lab": null,
      "concepts": [
        {
          "id": 594208,
          "key": "43b2cd98-a5ae-4eb7-b1a5-6cb0a5cee3c0",
          "title": "Project Introduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "43b2cd98-a5ae-4eb7-b1a5-6cb0a5cee3c0",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626195,
              "key": "88d20cf5-64d8-420b-8487-fa93a4fbc9f3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Build an Adversarial Game Playing Agent\n---\n",
              "instructor_notes": ""
            },
            {
              "id": 626197,
              "key": "87006ddb-3afa-491c-9679-efb640ea552f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5af117ff_viz/viz.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/87006ddb-3afa-491c-9679-efb640ea552f",
              "caption": "**NOTE**: The project is played on a 9x11 grid instead of a 7x7 grid.",
              "alt": "",
              "width": 480,
              "height": 318,
              "instructor_notes": null
            },
            {
              "id": 626196,
              "key": "99adb89f-0aeb-4385-beb3-d3bf14ffed0f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Synopsis\n\nIn this project, you will experiment with adversarial search techniques by building an agent to play knights Isolation.  Unlike the examples in lecture where the players control tokens that move like chess queens, this version of Isolation gives each agent control over a single token that moves in L-shaped movements--like a knight in chess.\n\n### Isolation\n\nIn the game Isolation, two players each control their own single token and alternate taking turns moving the token from one cell to another on a rectangular grid.  Whenever a token occupies a cell, that cell becomes blocked for the remainder of the game.  An open cell available for a token to move into is called a \"liberty\".  The first player with no remaining liberties for their token loses the game, and their opponent is declared the winner.\n\nIn knights Isolation, tokens can move to any open cell that is 2-rows and 1-column or 2-columns and 1-row away from their current position on the board.  On a blank board, this means that tokens have at most eight liberties surrounding their current location.  Token movement is blocked at the edges of the board (the board does not wrap around the edges), however, tokens can \"jump\" blocked or occupied spaces (just like a knight in chess).\n\nFinally, agents have a fixed time limit (150 milliseconds by default) to search for the best move and respond.  The search will be automatically cut off after the time limit expires, and the active agent will forfeit the game if it has not chosen a move.\n\n\n## Getting Started (Workspaces)\n\nThe easiest way to complete the project is to use the Udacity Workspace in your classroom. The environment has already been configured with the required files and libraries to support the project. If you decide to use the Workspace, then you do NOT need to perform any of the setup steps for this project. Skip to the section with instructions for completing the project.\n\n\n## Getting Started (Local Environment)\n\nIf you would prefer to complete the exercise in your own local environment, then follow the steps below:\n\n- Open your terminal and activate the aind conda environment (OS X or Unix/Linux users use the command shown; Windows users only run `activate aind`)\n```\n$ source activate aind\n```\n\n- Download a copy of the project files from GitHub and navigate to the project folder. (Note: if you've previously downloaded the repository for another project then you can skip the clone command. However, you should run `git pull` to receive any project updates since you cloned the repository.)\n```\n(aind) $ git clone https://github.com/udacity/artificial-intelligence\n(aind) $ cd \"artificial-intelligence/Projects/3_Game Playing\"\n```\n\n\n## Instructions\n\nYou must implement an agent in the `CustomPlayer` class defined in the `game_agent.py` file. The interface definition for game agents only requires you to implement the `.get_action()` method, but you can add any other methods to the class that you deem necessary.  You can build a basic agent by combining minimax search with alpha-beta pruning and iterative deepening from lecture.\n\n>**NOTE:** Your agent will **not** be evaluated in an environment suitable for running machine learning or deep learning agents (like AlphaGo); visit an office hours sessions **after** completing the project if you would like guidance on incorporating machine learning in your agent.\n\n#### The get_action() Method\nThis function is called once per turn for each player. The calling function handles the time limit and \n```\ndef get_action(self, state):\n    import random\n    self.queue.put(random.choice(state.actions()))\n```\n\n- **DO NOT** use multithreading/multiprocessing (the isolation library already uses them, which may cause conflicts)\n- **ALL** of the functions you add should be created as methods on the CustomPlayer class. \n\n#### Initialization Data\nYour agent will automatically read the contents of a file named `data.pickle` if it exists in the same folder as `my_custom_player.py`. The serialized object from the pickle file will be assigned to `self.data`. Your agent should not write to or modify the contents of the pickle file during search.\n\n\n#### Saving Information Between Turns\nThe `CustomPlayer` class can pass internal state by assigning the data to the attribute `self.context`. An instance of your agent class will carry the context between each turn of a single game, but the contents will be reset at the start of any new game.\n```\ndef get_action(...):\n    action = self.mcts()\n    self.queue.put(action)\n    self.context = object_you_want_to_save  # self.context will contain this object on the next turn\n```\n\n## Pick an Experiment\n\nSelect at least one of the following to implement and evaluate in your report. (There is no upper limit on the techniques you incorporate into your agent.)\n\n### Option 1: Develop a custom heuristic (must not be one of the heuristics from lectures, and cannot only be a combination of the number of liberties available to each agent)\n\n- Create a performance baseline using `run_search.py` (with the `fair_matches` flag enabled) to evaluate the effectiveness of your agent using the #my_moves - #opponent_moves heuristic from lecture\n- Use the same process to evaluate the effectiveness of your agent using your own custom heuristic\n    \n**Hints:**\n- Research other games (chess, go, connect4, etc.) to get ideas for developing good heuristics\n- If the results of your tests are very close, try increasing the number of matches (e.g., >100) to increase your confidence in the results\n- Experiment with adding more search time--does adding time confer any advantage to your agent over the baseline?\n- Augment the code to count the nubmer of nodes your agent searches--is it better to search more or fewer nodes? How does your heuristic compare to the baseline heuristic you chose?\n\n\n### Option 2: Develop an opening book (must span at least depth 4 of the search tree)\n\n- Write your own code to develop an opening book of the best moves for every possible game state from an empty board to at least a depth of 4 plies\n- Create a performance baseline using `run_search.py` (with the `fair_matches` flag _disabled_) to evaluate the effectiveness of your agent using randomly chosen opening moves.  (You can use any heuristic function, but you should use the same heuristic on your agent for all experiments.)\n- Use the same procedure to evaluate the effectiveness of your agent when early moves are selected from your opening book\n\n**Hints:**\n- Developing an opening book can require long run-times to simulate games and accumulate outcome statistics\n- If the results are very close, try increasing the number of matches (e.g., >100) to increase your confidence in the results\n\n**Adding a basic opening book**\n- You will need to write your own code to develop a good opening book, but you can pass data into your agent by saving the file as \"data.pickle\" in the same folder as `game_agent.py`. Use the [pickle](https://docs.python.org/3/library/pickle.html) module to serialize the object you want to save. The pickled object will be accessible to your agent through the `self.data` attribute.\n\nFor example, the contents of dictionary `my_data` can be saved to disk:\n```\nimport pickle\nfrom isolation import Isolation\nstate = Isolation()\nmy_data = {state: 57}  # opening book always chooses the middle square on an open board\nwith open(\"data.pickle\", 'wb') as f:\n    pickle.dump(my_data, f)\n```\n\n\n### Option 3: Build an agent using advanced search techniques (for example: killer heuristic, principle variation search (not in lecture), or monte carlo tree search (not in lecture))\n\n- Create a performance baseline using `run_search.py` to evaluate the effectiveness of a baseline agent (e.g., an agent using your minimax or alpha-beta search code from the classroom)\n- Use `run_search.py` to evaluate the effectiveness of your agent using your own custom search techniques\n- You must decide whether to test with or without \"fair\" matches enabled--justify your choice in your report\n    \n**Hints:**\n- If the results are very close, try increasing the number of matches (e.g., >100) to increase your confidence in the results\n- Experiment with adding more search time--does adding time confer any advantage to your agent?\n- Augment the code to count the number of nodes your agent searches--does your agent have an advantage compared to the baseline search algorithm you chose?\n\n>**Note:**\n- You MAY implement advanced techniques from the reading list at the end of the lesson (like Monte Carlo Tree Search, principal variation search, etc.), but your agent is being evaluated for _performance_ rather than _correctness_. It's possible to pass the project requirements **without** using these advanced techniques, so project reviewers may encourage you to implement a simpler solution if you are struggling with the correct implementation. (That's good general advice: do the simplest thing first, and only add complexity when you must.)\n\n\n## Report Requirements\n\nYour report must include a table or chart with data from an experiment to evaluate the performance of your agent as described above.  Use the data from your experiment to answer the relevant questions below. (You may choose one set of questions if your agent incorporates multiple techniques.)\n\n**Advanced Heuristic**\n- What features of the game does your heuristic incorporate, and why do you think those features matter in evaluating states during the search?\n- Analyze the search depth your agent achieves using your custom heuristic. Does search speed matter more or less than accuracy to the performance of your heuristic?\n\n**Opening book**\n- Describe your process for collecting statistics to build your opening book. How did you choose states to sample? And how did you perform rollouts to determine a winner?\n- What opening moves does your book suggest are most effective on an empty board for player 1 and what is player 2's best reply?\n\n**Advanced Search Techniques**\n- Choose a baseline search algorithm for comparison (for example, alpha-beta search with iterative deepening, etc.). How much performance difference does your agent show compared to the baseline?\n- Why do you think the technique you chose was more (or less) effective than the baseline?\n\n\n\n## Evaluation\n\nYour project will be reviewed by a Udacity reviewer against the project rubric [here](https://review.udacity.com/#!/rubrics/1801/view). Review this rubric thoroughly, and self-evaluate your project before submission. All criteria found in the rubric must meet specifications for you to pass.\n\n\n## Submission\n\nBefore you can submit your project for review in the classroom, you must run the remote test suite & generate a zip archive of the required project files. Submit the archive in your classroom for review. (See notes on submissions below for more details.) From your terminal, run the command: (make sure to activate the aind conda environment if you're running the project in your local environment; workspace users do **not** need to activate an environment.)\n```\n$ udacity submit\n```\nThe script will automatically create a zip archive of the required files (`my_custom_player.py` and `report.pdf` are required; `data.pickle` will be included if it exists) and submit your code to a remote server for testing. You can only submit a zip archive created by the PA script (even if you're only submitting a partial solution), and you **must submit the exact zip file created by the Project Assistant** in your classroom for review. The classroom verifies the zip file submitted against records on the Project Assistant system; any changes in the file will cause your submission to be rejected.\n\n>**NOTE:** Students who authenticate with Facebook or Google accounts _must_ follow the instructions on the FAQ page [here](https://project-assistant.udacity.com/faq) to obtain an authentication token. (The Workspace already includes instructions for obtaining and configuring your token.)\n",
              "instructor_notes": ""
            },
            {
              "id": 1104570,
              "key": "75c7d6ee-2474-4977-8997-b46f1ca46c36",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": ">**NOTE:** Importing some modules such as `logging` is currently not supported by the Autograder. If you do import some extra modules for debugging, etc., turn them off while submitting to the classroom. \n>Here is an example that shows how we can define a flag `LOGGING_AVAILABLE`. For debugging, the value of this flag is set to `True`. \n\n```\nLOGGING_AVAILABLE = True\n\nif LOGGING_AVAILABLE:\n    import logging\n\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    fh = logging.FileHandler('stats.csv', mode='w')\n    fh.setLevel(logging.DEBUG)\n    logger.addHandler(fh)\n```\n>The piece of code shown above will cause an error that comes from the Autograder. To avoid the error, set the value of the`LOGGING_AVAILABLE` flag to `False` before submitting the project to the Autograder.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 624226,
          "key": "8363fdd0-33df-4f7c-9559-75a1b13c9fa9",
          "title": "Workspace",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8363fdd0-33df-4f7c-9559-75a1b13c9fa9",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624227,
              "key": "c0d4bcca-e22d-45a2-b787-3d8a1731d657",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view1f1dfbf6",
              "pool_id": "webterminal",
              "view_id": "1f1dfbf6-9244-4b69-afce-935503b02fc3",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "port": 3000,
                    "ports": [],
                    "userCode": "",
                    "openFiles": [
                      "/home/workspace/jwt",
                      "/home/workspace/my_custom_player.py"
                    ],
                    "showFiles": true,
                    "allowClose": true,
                    "allowSubmit": true
                  },
                  "kind": "generic"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "starter_files",
      "reason": "prefer master_archive_id"
    }
  ]
}