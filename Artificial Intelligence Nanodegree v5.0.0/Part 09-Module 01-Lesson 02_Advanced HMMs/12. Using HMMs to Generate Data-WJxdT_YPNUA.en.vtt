WEBVTT
Kind: captions
Language: en

00:00:00.072 --> 00:00:03.351
One last thing I'd like to cover is
why the normal HMM formulation is not

00:00:03.351 --> 00:00:04.910
good for generating data.

00:00:04.910 --> 00:00:08.375
In the early days of speech recognition,
there was the hope that

00:00:08.375 --> 00:00:12.431
we could use the same HMMs we use to
recognize speech, to also generate it.

00:00:12.430 --> 00:00:13.702
It turned out not to
be such a good idea.

00:00:13.702 --> 00:00:16.096
Even though later advancements
led to good results.

00:00:16.096 --> 00:00:18.835
&gt;&gt; Maybe an example is the best
way to show the issue.

00:00:18.835 --> 00:00:19.539
&gt;&gt; You're right.

00:00:19.539 --> 00:00:22.521
Let's take this same HMM we had
at the beginning of the lesson.

00:00:22.521 --> 00:00:24.387
&gt;&gt; The one which modeled
the stray lines?

00:00:24.388 --> 00:00:28.719
&gt;&gt; Yep, now with just looking at
the HMM, not the example data,

00:00:28.719 --> 00:00:33.148
give me ten numbers that could
be generated by the first state.

00:00:33.148 --> 00:00:37.711
&gt;&gt; Okay, -1, -1, -2, -1.5,

00:00:37.710 --> 00:00:42.957
-1, -1.25, -1.75, and -2.

00:00:42.957 --> 00:00:44.586
&gt;&gt; Now some numbers for
the second state.

00:00:44.587 --> 00:00:50.598
&gt;&gt; 0, -0.5, -0.25, -0.75 and -1.

00:00:50.597 --> 00:00:52.554
&gt;&gt; Same thing for the third state.

00:00:52.554 --> 00:00:57.417
&gt;&gt; 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,

00:00:57.417 --> 00:01:00.968
0, 1, 0, 0, 1, 0, 1, 0.

00:01:00.968 --> 00:01:02.629
&gt;&gt; And finally the last state.

00:01:02.628 --> 00:01:06.376
&gt;&gt; 1.5, 1.75 and 1.

00:01:06.376 --> 00:01:09.548
&gt;&gt; Plotting these numbers looks nothing
like the original example because

00:01:09.549 --> 00:01:11.947
the output distributions
have no idea of continuity.

00:01:11.947 --> 00:01:14.007
&gt;&gt; How would we fix that?

00:01:14.007 --> 00:01:17.304
&gt;&gt; We could use a lot of states to
try to force a better ordering of

00:01:17.304 --> 00:01:20.189
the output, but
that would lead to over-fitting.

00:01:20.189 --> 00:01:23.057
A better idea is to model
the state transitions with an HMM.

00:01:23.058 --> 00:01:26.402
But use a different process while
in each state that is more aware of

00:01:26.402 --> 00:01:28.960
the context to generate the final data.

00:01:28.959 --> 00:01:30.920
There are several methods
that are possible.

00:01:30.921 --> 00:01:34.999
But the most sophisticated work I know
of is by speech synthesis researchers.

00:01:34.998 --> 00:01:38.537
For example, Google has some good
documents of how they are combining HMMs

00:01:38.537 --> 00:01:40.909
with deep belief networks
to get better results.最新课程跟课件还有一对一辅导请加wx：udacity6

