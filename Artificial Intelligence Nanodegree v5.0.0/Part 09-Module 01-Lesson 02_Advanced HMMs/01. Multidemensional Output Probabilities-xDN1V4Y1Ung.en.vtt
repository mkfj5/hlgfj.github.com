WEBVTT
Kind: captions
Language: en

00:00:00.450 --> 00:00:02.719
Now that we've shown how HMMs work,

00:00:02.718 --> 00:00:05.389
let's provide some more tips
on how to improve them.

00:00:05.389 --> 00:00:08.830
&gt;&gt; Okay, in our example of using HMMs
to distinguish between the signs

00:00:08.830 --> 00:00:11.820
I versus we,
we used delta y as a feature.

00:00:11.820 --> 00:00:14.740
But in reality delta x
would be a better feature.

00:00:14.740 --> 00:00:18.833
&gt;&gt; That's true, but for other signs
delta y would be a good feature.

00:00:18.833 --> 00:00:23.211
Another good feature is the size of the
hand which helps us get some information

00:00:23.211 --> 00:00:26.210
as to if the hand is coming
towards the camera or away.

00:00:26.210 --> 00:00:29.997
&gt;&gt; Another good feature might be the
angle the hand makes to the horizontal.

00:00:29.998 --> 00:00:31.420
&gt;&gt; So sign language is two handed,

00:00:31.420 --> 00:00:34.640
we should have these features
both to the right and left hands.

00:00:34.640 --> 00:00:37.923
Now that we have eight features,
we are tracking for time frame,

00:00:37.923 --> 00:00:39.903
how do we integrate it into our models?

00:00:39.904 --> 00:00:41.662
&gt;&gt; Actually, it's pretty easy.

00:00:41.661 --> 00:00:44.460
We just add more dimensions for
the output probabilities.

00:00:44.460 --> 00:00:47.149
All our training and
recognition work like before.

00:00:47.149 --> 00:00:49.789
We just have to calculate
multi-dimensional distances

00:00:49.789 --> 00:00:51.950
instead of using just one dimension.

00:00:51.950 --> 00:00:52.710
&gt;&gt; You're right.

00:00:52.710 --> 00:00:56.020
It gets hard to show on our graphs
here but it's easy enough to code.

