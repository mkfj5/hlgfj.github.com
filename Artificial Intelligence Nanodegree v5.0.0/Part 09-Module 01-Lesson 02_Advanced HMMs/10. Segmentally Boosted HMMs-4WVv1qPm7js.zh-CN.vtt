WEBVTT
Kind: captions
Language: zh-CN

00:00:00.400 --> 00:00:02.290
在你之前研究手势识别时

00:00:02.290 --> 00:00:05.169
你曾经使用过多少维度计算输出效率呢？

00:00:05.169 --> 00:00:06.299
— 多达几百个

00:00:06.299 --> 00:00:08.900
在某一时刻 我们创建了手的外观模型

00:00:08.900 --> 00:00:12.380
使用相似性度量 查看手的相似度

00:00:12.380 --> 00:00:15.400
手的视觉模型作为隐马尔可夫模型的特征

00:00:15.400 --> 00:00:18.820
不过问题是无法匹配的模型中存在大量噪音

00:00:18.820 --> 00:00:22.179
给出相对随机的结果

00:00:22.179 --> 00:00:25.259
因为许多维度没有意义 除非手

00:00:25.260 --> 00:00:26.910
在某个时刻匹配良好

00:00:26.910 --> 00:00:29.839
— 那么所有维度实际上是上海 因为它们

00:00:29.839 --> 00:00:31.160
大多数时候都很嘈杂？

00:00:31.160 --> 00:00:34.740
— 对 不过我们可以使用提升算法 帮助我们权衡特征向量

00:00:34.740 --> 00:00:36.080
解决这个问题

00:00:36.079 --> 00:00:39.000
这个技巧叫做局部提升隐马尔可夫模型

00:00:39.000 --> 00:00:43.140
我有一位学生针对这个问题做了博士论文

00:00:43.140 --> 00:00:46.420
他的结果比普通隐马尔可夫模型提升了 20%

00:00:46.420 --> 00:00:49.359
对于一些数据集 我们可提升 70%

00:00:49.359 --> 00:00:50.820
— 怎么做到的呢？

00:00:50.820 --> 00:00:54.490
— 首先我们照常对齐并训练隐马尔可夫模型

00:00:54.490 --> 00:00:57.780
接下来我们使用训练 尽量对齐

00:00:57.780 --> 00:00:59.870
属于每个数据的数据

00:00:59.869 --> 00:01:03.169
我们不断在每个模型中检查每个状态

00:01:03.170 --> 00:01:07.320
通过提问哪些特征有利于帮助我们区分

00:01:07.319 --> 00:01:10.879
已选状态和剩余状态的数据 进行提升

00:01:10.879 --> 00:01:14.359
然后在隐马尔可夫模型中 合理地权衡维度

00:01:14.359 --> 00:01:17.269
这个技巧综合了判别模型

00:01:17.269 --> 00:01:18.810
和衍生法的一些优势

00:01:18.810 --> 00:01:20.710
— 它也在工具包里吗？

00:01:20.709 --> 00:01:21.449
— 还没有

00:01:21.450 --> 00:01:26.129
不过我在考虑把它添加到 HTK 和乔治亚理工大学手势工具包中

00:01:26.129 --> 00:01:29.769
从个人角度讲 我认为它非常强大 但是并没有广为人知

