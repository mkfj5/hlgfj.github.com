WEBVTT
Kind: captions
Language: zh-CN

00:00:00.420 --> 00:00:00.950
好

00:00:00.950 --> 00:00:02.790
我们现在来讨论另一个技巧

00:00:02.790 --> 00:00:07.540
我们从识别孤立的手势转为识别手势短语

00:00:07.540 --> 00:00:10.429
动作组合看起来非常不同

00:00:10.429 --> 00:00:12.949
— 例如 Thad 用手势单独表示 NEED 时

00:00:12.949 --> 00:00:16.550
他的双手从静止的位置开始 从静止的位置结束

00:00:16.550 --> 00:00:19.469
当他表示 I NEED CAT 语境中的 NEED 时

00:00:19.469 --> 00:00:22.169
NEED 第一部分跟在 I 最后一部分后面

00:00:22.170 --> 00:00:24.870
NEED 最后一部分后面紧跟着 CAT 第一部分

00:00:24.870 --> 00:00:27.220
他的双手不再移动幅度很大

00:00:27.219 --> 00:00:29.250
— 没错 在既定手势前后的手势

00:00:29.250 --> 00:00:32.618
可以对如何表示它具有重大影响

00:00:32.618 --> 00:00:35.119
— 所以这里不是识别 NEED 的三个状态模型

00:00:35.119 --> 00:00:40.059
我们关注 I NEED 和 NEED CAT 的六个组合状态模型

00:00:40.060 --> 00:00:43.387
— 在实际中 我们对许多手势短语

00:00:43.387 --> 00:00:45.149
而不是单个手语的短语示例 进行训练

00:00:45.149 --> 00:00:49.314
我最初针对识别美式手语的论文中 我有 40 个词语的手语

00:00:49.314 --> 00:00:53.763
但是训练的数据是 500 个短语

00:00:53.764 --> 00:00:56.160
每个包括 5 个手势

00:00:56.159 --> 00:00:56.785
— 在这种情况下

00:00:56.786 --> 00:00:59.667
假设我们包括 3 个手势短语的许多例子

00:00:59.667 --> 00:01:04.376
在我们隐马尔可夫模型数据训练的例子中 我们假设模型 I 的

00:01:04.376 --> 00:01:06.478
单独手势数据

00:01:06.477 --> 00:01:09.250
平均分布在三个状态中

00:01:09.250 --> 00:01:12.599
然后我们根据假设计算输出概率

00:01:12.599 --> 00:01:15.039
调整分界线和转移概率

00:01:15.040 --> 00:01:16.880
然后迭代 直到收敛到一点

00:01:16.879 --> 00:01:20.109
— 我们第一步同样这么做

00:01:20.109 --> 00:01:24.510
但是这次我们假设数据平均分布在每个手势中

00:01:24.510 --> 00:01:27.940
并且我们对每个手势中各状态的数据进行切分

00:01:27.939 --> 00:01:29.799
— 然后与之前一样进行迭代

00:01:29.799 --> 00:01:33.579
调整每个状态和手势的分界线 直到收敛到一点

00:01:33.579 --> 00:01:35.469
— 现在事情更加有趣了

00:01:35.469 --> 00:01:39.170
我们收敛了每个手势的内容 然后回去

00:01:39.170 --> 00:01:41.680
发现 I NEED 出现的每一处

00:01:41.680 --> 00:01:46.190
注意 这里比 NEED 例子中少很多

00:01:46.189 --> 00:01:48.554
— 不过我们认为有足够多的例子

00:01:48.555 --> 00:01:49.395
— 好

00:01:49.394 --> 00:01:52.804
我们要删除我们认为属于 I NEED 中的数据

00:01:52.805 --> 00:01:54.985
训练 6 个组合状态模型

00:01:54.984 --> 00:01:56.572
— 那样有什么用途呢？

00:01:56.572 --> 00:01:59.414
— I 和 NEED 分界线的输出概率

00:01:59.415 --> 00:02:01.845
这儿和这儿

00:02:01.844 --> 00:02:06.385
以及这个区域的转移概率可以进行调整

00:02:06.385 --> 00:02:11.259
比包括 we need 的一般情况中 更好地表现 I NEED

00:02:11.259 --> 00:02:16.004
在语音中 影响临近音素的某个音素效果

00:02:16.004 --> 00:02:21.129
称为协同发音 这种建模方法叫做上下文训练

00:02:21.129 --> 00:02:24.439
— 我发现我们要对 NEED CAT1 进行相同的操作

00:02:24.439 --> 00:02:26.983
— 没错 对于其他两个手势组合

00:02:26.983 --> 00:02:31.605
NEED 和 CAT2 WANT 和 CAT1 WANT 和 CAT2 I 和 WANT WE 和
NEED 以及 WE 和 WANT

00:02:31.604 --> 00:02:35.326
我们使用波氏重估进行迭代 从嵌入式训练中使用更大的语境

00:02:35.326 --> 00:02:38.439
直到我们再次收敛到一点

00:02:38.439 --> 00:02:40.530
— 为什么不使用三个手势语境呢？

00:02:40.530 --> 00:02:42.960
或者是短语更加复杂的情况呢？

00:02:42.960 --> 00:02:45.060
— 如果我们有足够多的数据 这是个不错的主意

00:02:45.060 --> 00:02:47.770
因为优点实际上更大

00:02:47.770 --> 00:02:50.560
对于识别任务 对于语言结构的地方

00:02:50.560 --> 00:02:53.289
我们希望进行上下文训练 把误差率降到一半

