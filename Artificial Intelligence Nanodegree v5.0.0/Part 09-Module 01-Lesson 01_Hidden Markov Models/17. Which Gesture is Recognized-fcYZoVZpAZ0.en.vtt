WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:02.650
So it looks like it's a lot more
probable that the model for

00:00:02.649 --> 00:00:04.450
I generated this data.

00:00:04.450 --> 00:00:04.810
&gt;&gt; Yep.

00:00:04.809 --> 00:00:07.660
The main difference between the values
for the models producing this

00:00:07.660 --> 00:00:11.169
observation sequence has to
do with the middle state.

00:00:11.169 --> 00:00:14.429
Remember that we used delta y even
though it is a relatively bad

00:00:14.429 --> 00:00:17.140
feature for distinguishing I from we.

00:00:17.140 --> 00:00:19.231
The output probability Gaussian for

00:00:19.231 --> 00:00:22.477
the middle state of both models
have a mean value of zero.

00:00:22.477 --> 00:00:27.507
However, with I, the expected
probability of getting an actual zero at

00:00:27.507 --> 00:00:32.467
the middle state is much higher
than with we, a 0.9 versus a 0.7.

00:00:32.468 --> 00:00:33.717
&gt;&gt; Also with the gesture I,

00:00:33.716 --> 00:00:36.979
we spend much less time in
the middle state than with We.

00:00:36.979 --> 00:00:40.750
The transition probabilities for the
middle states reflect this difference.

00:00:40.750 --> 00:00:44.170
With I, our transition from the middle
state to the last state is 0.5 but

00:00:45.210 --> 00:00:46.695
with We it was 0.3.

00:00:46.695 --> 00:00:50.954
This example shows how well HMMs can
distinguish between two gestures,

00:00:50.954 --> 00:00:52.684
even with relatively poor features.

00:00:52.685 --> 00:00:56.565
&gt;&gt; What is great about HMMs,
is that the difference in values for

00:00:56.564 --> 00:01:00.375
even relatively weak features,
accumulates through time.

00:01:00.375 --> 00:01:04.424
The longer the gesture is,
the easier it is to recognize.

00:01:04.424 --> 00:01:06.484
Perhaps we can experiment
with that idea in a quiz.

